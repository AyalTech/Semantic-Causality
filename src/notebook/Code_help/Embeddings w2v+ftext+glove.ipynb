{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE!\n",
    "# Evaluate word embeddings on different dimension numbers\n",
    "# ------\n",
    "#import nltk\n",
    "#nltk.data.path.insert(0, \"/home/saffeldt/Projects/Projects_we/source/we_ensemble/data/nltk_data\")\n",
    "#nltk.download('brown', download_dir = \"/home/saffeldt/Projects/Projects_we/source/we_ensemble/data/nltk_data\")\n",
    "#print(nltk.data.path)\n",
    "#print(nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE!\n",
    "## Preprocess Brown corpus\n",
    "#from gensim.parsing.preprocessing import strip_punctuation, strip_multiple_whitespaces\n",
    "#import os\n",
    "#import zipfile\n",
    "#import wget\n",
    "#\n",
    "## Generate brown corpus text file\n",
    "#if not os.path.isfile('../data/nltk_data/brown_corp.txt'):\n",
    "# with open('../data/nltk_data/brown_corp.txt', 'w+') as f:\n",
    "# print(os.path.realpath(f.name))\n",
    "# for word in nltk.corpus.brown.words():\n",
    "# f.write('{word} '.format(word = word))\n",
    "# f.seek(0)\n",
    "# brown = f.read()\n",
    "#\n",
    "## Preprocess brown corpus\n",
    "#if not os.path.isfile('../data/nltk_data/proc_brown_corp.txt'):\n",
    "# with open('../data/nltk_data/proc_brown_corp.txt', 'w') as f:\n",
    "# proc_brown = strip_punctuation(brown)\n",
    "# proc_brown = strip_multiple_whitespaces(proc_brown).lower()\n",
    "# f.write(proc_brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Download the text8 corpus (a 100 MB sample of preprocessed\n",
    "# wikipedia text)\n",
    "if not os.path.isfile('../data/text8'):\n",
    "    #!wget -c http://mattmahoney.net/dc/text8.zip\n",
    "    wget.download('http://mattmahoney.net/dc/text8.zip','../data/text8.zip')\n",
    "\n",
    "    #!unzip text8.zip\n",
    "    archive = zipfile.ZipFile('../data/text8.zip')\n",
    "    archive.extractall('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set WR_HOME et FT_HOME to respective directory root\n",
    "#FT_HOME = '../fastText/'\n",
    "# Train models\n",
    "# ------\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "import os\n",
    "import itertools\n",
    "from glove import Corpus, Glove\n",
    "lr = 0.025\n",
    "window = 5\n",
    "nbThreads = 3\n",
    "minCount = 50\n",
    "t = 0\n",
    "n_iter = 10\n",
    "# word2vec\n",
    "w2v_params = {\n",
    "    'alpha': lr,\n",
    "    'window': window,\n",
    "    'iter': n_iter, # Nbr epochs\n",
    "    'min_count': minCount,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 0,\n",
    "    'negative': 5\n",
    "}\n",
    "# fasttext\n",
    "ft_params = {\n",
    "    'alpha': lr,\n",
    "    'window': window,\n",
    "    'iter': n_iter, # Nbr epochs\n",
    "    'min_count': minCount,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 0,\n",
    "    'negative': 5\n",
    "}\n",
    "# Glove parameters\n",
    "gv_lr = lr\n",
    "gv_window = window\n",
    "MODELS_DIR = '../data/models/'\n",
    "!mkdir -p {MODELS_DIR}\n",
    "def train_w2v(corpus_file, output_name, dim):\n",
    "    # Train using word2vec\n",
    "    output_file = '{:s}_wv'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}{:d}dim.vec'.format(output_file, dim))):\n",
    "        \n",
    "        print('\\nTraining word2vec on {:s} corpus..'.format(corpus_file))\n",
    "        # Text8Corpus class for reading space-separated words file\n",
    "        %time wv_model = Word2Vec(Text8Corpus(corpus_file), size = dim, **w2v_params); wv_model\n",
    "        locals()['wv_model'].wv.save_word2vec_format(os.path.join(MODELS_DIR,'{:s}{:d}dim.vec'.format(output_file, dim)))\n",
    "        print('\\nSaved word2vec model as {:s}{:d}dim.vec'.format(output_file, dim))\n",
    "        \n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}{:d}dim.vec'.format(output_file, dim))\n",
    "        \n",
    "def train_ft(corpus_file, output_name, dim):\n",
    "    # Train using fasttext\n",
    "    #output_file = '{:s}_ft'.format(output_name)\n",
    "    #if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}{:d}dim.vec'.format(output_file, dim))):\n",
    "    # print('\\nTraining fasttest on {:s} corpus..'.format(corpus_file))\n",
    "    # #%time !{FT_HOME}fasttext skipgram -input {corpus_file} -output {MODELS_DIR+output_file} -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n",
    "    # tmp_output_file = os.path.join(MODELS_DIR, '{:s}{:d}dim'.format(output_file, dim))\n",
    "    # %time !{FT_HOME}fasttext skipgram -input {corpus_file} -output {tmp_output_file} -lr {lr} -dim {dim} -ws {ws} -epoch {ft_iter} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n",
    "    # print('\\nSaved fasttext model as {:s}{:d}dim.vec'.format(output_file, dim))\n",
    "    #else:\n",
    "    # print('\\nUsing existing model file {:s}{:d}dim.vec'.format(output_file, dim))\n",
    "    output_file = '{:s}_ft'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}{:d}dim.vec'.format(output_file, dim))):\n",
    "        \n",
    "        print('\\nTraining fasttest on {:s} corpus..'.format(corpus_file))\n",
    "        # Text8Corpus class for reading space-separated words file\n",
    "        %time ft_model = FastText(sentences = Text8Corpus(corpus_file), size = dim, **ft_params); ft_model\n",
    "        locals()['ft_model'].wv.save_word2vec_format(os.path.join(MODELS_DIR,'{:s}{:d}dim.vec'.format(output_file, dim)))\n",
    "        print('\\nSaved fasttext model as {:s}{:d}dim.vec'.format(output_file, dim))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print('\\nUsing existing model file {:s}{:d}dim.vec'.format(output_file, dim))\n",
    "\n",
    "\n",
    "\n",
    "def train_gv(corpus, corpus_file, output_name, dim):\n",
    "    # Train using GloVe\n",
    "    output_file = '{:s}_gv'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}{:d}dim.vec'.format(output_file, dim))):\n",
    "        print('\\nTraining GloVe on {:s} corpus..'.format(corpus_file))\n",
    "        glove = Glove(no_components = dim, learning_rate = gv_lr)\n",
    "        glove.fit(corpus.matrix, epochs = n_iter, no_threads = nbThreads, verbose = True)\n",
    "        glove.add_dictionary(corpus.dictionary)\n",
    "        # Sort words by their id\n",
    "        word_and_word_id_tuples = list(glove.dictionary.items())\n",
    "        word_and_word_id_tuples.sort(key=lambda t: t[1]) # id is second element of tuple\n",
    "        # Save vectors to output file\n",
    "        with open(os.path.join(MODELS_DIR, '{:s}{:d}dim.vec'.format(output_file, dim)), 'w+') as f:\n",
    "            f.write(str(len(word_and_word_id_tuples)) + \" \" + str(dim) + \"\\n\")\n",
    "                for word, word_id in word_and_word_id_tuples:\n",
    "                f.write(word + \" \" + \" \".join([str(value) for value in glove.word_vectors[word_id]]) + \"\\n\")\n",
    "        print('\\nSaved GloVe model as {:s}{:d}dim'.format(output_file, dim))\n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}{:d}dim'.format(output_file, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the corpus variable for GloVe\n",
    "corpus = dict()\n",
    "sentences = dict()\n",
    "## Brown\n",
    "#corpus_file = '../data/nltk_data/proc_brown_corp.txt'\n",
    "#corpus_name = 'brown'\n",
    "#sentences[corpus_name] = list(itertools.islice(Text8Corpus(corpus_file),None))\n",
    "#corpus[corpus_name]= Corpus()\n",
    "#corpus[corpus_name].fit(sentences[corpus_name], window = window)\n",
    "# Text8\n",
    "#corpus_file = '../data/text8'\n",
    "corpus_file = '../data/proc_text8_corp_clean.txt'\n",
    "corpus_name = 'text8'\n",
    "sentences[corpus_name] = list(itertools.islice(Text8Corpus(corpus_file), None))\n",
    "corpus[corpus_name] = Corpus()\n",
    "corpus[corpus_name].fit(sentences[corpus_name], window = gv_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus[corpus_name].dictionary)\n",
    "\n",
    "len(dict(list(zip(all_models['100'].index2word, range(len(all_models['100'].index2word))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dim in [50, 100, 150, 200, 250]:\n",
    "for dim in [100, 150, 200, 250]:\n",
    "    ## Train word2vec, fast text, GloVe on Brown\n",
    "    #corpus_file = '../data/nltk_data/proc_brown_corp.txt'\n",
    "    #corpus_name = 'brown'\n",
    "    #train_w2v(corpus_file = corpus_file, output_name = corpus_name, dim = dim)\n",
    "    #train_ft(corpus_file = corpus_file, output_name = corpus_name, dim = dim)\n",
    "    #train_gv(corpus = corpus[corpus_name], corpus_file = corpus_file, output_name = corpus_name, dim = dim)\n",
    "    # Train word2vec, fast text, GloVe on Text8\n",
    "    #corpus_file = '../data/text8'\n",
    "    corpus_file = '../data/proc_text8_corp_clean.txt'\n",
    "    corpus_name = 'text8'\n",
    "    train_w2v(corpus_file = corpus_file, output_name = corpus_name, dim = dim)\n",
    "    train_ft(corpus_file = corpus_file, output_name = corpus_name, dim = dim)\n",
    "    train_gv(corpus = corpus[corpus_name], corpus_file = corpus_file, output_name = corpus_name,dim = dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "\n",
    "# ------\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "\n",
    "level = logging.INFO)\n",
    "\n",
    "# wordsim_sim\n",
    "wordsim_sim_file = \"../eval_files/wordsim_similarity_goldstandard.txt\"\n",
    "# wordsim_rel\n",
    "wordsim_rel_file = \"../eval_files/wordsim_relatedness_goldstandard.txt\"\n",
    "# wordsim_353\n",
    "wordsim_file = \"../eval_files/wordsim353.tsv\"\n",
    "# men\n",
    "men_file = \"../eval_files/MEN_dataset_natural_form_full.tsv\"\n",
    "# mturk\n",
    "mturk_file = \"../eval_files/mturk-771.tsv\"\n",
    "# simlex\n",
    "simlex_file = \"../eval_files/simlex999.txt\"\n",
    "# analogy\n",
    "word_analogies_file = \"../eval_files/questions-words.txt\"\n",
    "def print_analogy_accuracy(model, questions_file):\n",
    "    acc = model.evaluate_word_analogies(questions_file,dummy4unknown=False)\n",
    "    # Semantic:\n",
    "    # [0] capital-common-countries, [1] capital-world\n",
    "    # [2] currency, [3] city-in-state, [4] family\n",
    "    n_sem = 5\n",
    "    sem_correct = sum((len(acc[1][i]['correct']) for i in range(n_sem)))\n",
    "    sem_total = sum((len(acc[1][i]['correct']) + len(acc[1][i]['incorrect'])) for i in range(n_sem))\n",
    "    sem_acc = 100*float(sem_correct)/sem_total\n",
    "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
    "    # Syntactic:\n",
    "    # [5] gram1-adjective-to-adverb, [6] gram2-opposite, [7] gram3-comparative,\n",
    "    # [8] gram4-superlative, [9] gram5-present-participle, [10] gram6-nationality-adjective,\n",
    "    # [11] gram7-past-tense, [12] gram8-plural, [13] gram9-plural-verbs\n",
    "    syn_correct = sum((len(acc[1][i]['correct']) for i in range(n_sem, len(acc[1])-1)))\n",
    "    syn_total = sum((len(acc[1][i]['correct']) + len(acc[1][i]['incorrect'])) for i in range(n_sem,len(acc[1])-1))\n",
    "    syn_acc = 100*float(syn_correct)/syn_total\n",
    "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
    "    return sem_acc, syn_acc\n",
    "def print_similarity_accuracy(model, similarity_file):\n",
    "    acc = model.evaluate_word_pairs(similarity_file, dummy4unknown=False)\n",
    "    #print('Pearson correlation coefficient: {:.2f}'.format(acc[0][0]))\n",
    "    print('Spearman rank correlation coefficient: {:.2f}'.format(acc[1][0]))\n",
    "    #return acc[0][0], acc[1][0]\n",
    "    return acc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE !!!!\n",
    "## Load and evaluate the Word2Vec embeddings\n",
    "#print('\\nLoading Gensim embeddings')\n",
    "#brown_wv = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_wv.vec')\n",
    "#\n",
    "## Brown: Explore the Word2Vec embeddings\n",
    "#\n",
    "## get the word vector of \"the\"\n",
    "##brown_wv['the']\n",
    "#\n",
    "## get the most common words\n",
    "#print(brown_wv.index2word[0],\n",
    "# brown_wv.index2word[1],\n",
    "# brown_wv.index2word[2])\n",
    "#\n",
    "## get the least common words\n",
    "#vocab_size = len(brown_wv.vocab)\n",
    "#print(vocab_size)\n",
    "#print(brown_wv.index2word[vocab_size - 1],\n",
    "# brown_wv.index2word[vocab_size - 2],\n",
    "# brown_wv.index2word[vocab_size - 3])\n",
    "#\n",
    "## find the index of the 2nd most common word (\"of\")\n",
    "#print('Index of \"of\" is: {}'.format(brown_wv.vocab['of'].index))\n",
    "#\n",
    "## some similarity fun\n",
    "#print(brown_wv.similarity('woman', 'man'),\n",
    "# brown_wv.similarity('man', 'elephant'))\n",
    "#\n",
    "## what doesn't fit?\n",
    "#print(brown_wv.doesnt_match(\"green blue red zebra\".split()))\n",
    "#\n",
    "#import numpy as np\n",
    "## convert the word vectors into a numpy matrix that is\n",
    "## suitable for insertion into our TensorFlow and Keras models\n",
    "#embedding_matrix_brown_wv = np.zeros((vocab_size, vector_dim))\n",
    "#for i in range(vocab_size):\n",
    "# embedding_vector = brown_wv[brown_wv.index2word[i]]\n",
    "# if embedding_vector is not None:\n",
    "# embedding_matrix_brown_wv[i] = embedding_vector\n",
    "#print(embedding_matrix_brown_wv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def we_evaluation(output_name, method, dim):\n",
    "\n",
    "    output_file = '{:s}_{:s}'.format(output_name, method)\n",
    "    print('\\nLoading {:s} embeddings (dim = {:d}, meth = {:s})'.format(output_name, dim, method))\n",
    "    tmp_model = KeyedVectors.load_word2vec_format(MODELS_DIR + '{:s}{:d}dim.vec'.format(output_file, dim))\n",
    "    tmp_eval = list()\n",
    "    print(\"\\nWordSim-353 similarity\")\n",
    "    ret_corSpearman_word_sim = print_similarity_accuracy(tmp_model, wordsim_sim_file)\n",
    "    tmp_eval.append(ret_corSpearman_word_sim)\n",
    "    print(\"\\nWordSim-353 relatedness\")\n",
    "    ret_corSpearman_word_rel = print_similarity_accuracy(tmp_model, wordsim_rel_file)\n",
    "    tmp_eval.append(ret_corSpearman_word_rel)\n",
    "    print(\"\\nWordSim-353\")\n",
    "    ret_corSpearman_word = print_similarity_accuracy(tmp_model, wordsim_file)\n",
    "    tmp_eval.append(ret_corSpearman_word)\n",
    "    print(\"\\nMEN similarity\")\n",
    "    ret_corSpearman_men = print_similarity_accuracy(tmp_model, men_file)\n",
    "    tmp_eval.append(ret_corSpearman_men)\n",
    "    print(\"\\nMturk similarity\")\n",
    "    ret_corSpearman_men = print_similarity_accuracy(tmp_model, mturk_file)\n",
    "    tmp_eval.append(ret_corSpearman_men)\n",
    "    print('SimLex-999 similarity')\n",
    "    ret_corSpearman_simlex = print_similarity_accuracy(tmp_model, simlex_file)\n",
    "    tmp_eval.append(ret_corSpearman_simlex)\n",
    "    print('Accuracy:')\n",
    "    ret_sem_acc, ret_syn_acc = print_analogy_accuracy(tmp_model, word_analogies_file)\n",
    "    tmp_eval.append(ret_sem_acc)\n",
    "    tmp_eval.append(ret_syn_acc)\n",
    "    return tmp_eval\n",
    "\n",
    "eval_all = dict()\n",
    "#for cp_name in ['brown', 'text8']:\n",
    "for cp_name in ['text8']:\n",
    "    output_name = cp_name\n",
    "    eval_all[cp_name] = list()\n",
    "    # Load and evaluate the Word2Vec embeddings\n",
    "    for dim in [50, 100, 150, 200, 250]:\n",
    "        #for dim in [50]:\n",
    "        \n",
    "        # Load and evaluate the Word2Vec embeddings\n",
    "        method = 'wv'\n",
    "        ret_eval = we_evaluation(output_name, method, dim)\n",
    "        eval_all[cp_name].append(ret_eval)\n",
    "        \n",
    "        # Load and evaluate the FastText embeddings\n",
    "        method = 'ft'\n",
    "        ret_eval = we_evaluation(output_name, method, dim)\n",
    "        eval_all[cp_name].append(ret_eval)\n",
    "        \n",
    "        # Load and evaluate the FastText embeddings\n",
    "        method = 'gv'\n",
    "        ret_eval = we_evaluation(output_name, method, dim)\n",
    "        eval_all[cp_name].append(ret_eval)\n",
    "        \n",
    "print(eval_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_all['text8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def color_negative_red(val):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: red'` for negative\n",
    "    strings, black otherwise.\n",
    "    \"\"\"\n",
    "    color = 'red' if val < 0 else 'black'\n",
    "    return 'color: %s' % color\n",
    "def highlight_max(data, color='yellow'):\n",
    "    '''\n",
    "    highlight the maximum in a Series or DataFrame\n",
    "    '''\n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    if data.ndim == 1: # Series from .apply(axis=0) or axis=1\n",
    "        is_max = data == data.max()\n",
    "        return [attr if v else '' for v in is_max]\n",
    "\n",
    "    else: # from .apply(axis=None)\n",
    "        is_max = data == data.max().max()\n",
    "        return pd.DataFrame(np.where(is_max, attr, ''),index=data.index, columns=data.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_all[cp_name][2][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table_oneDim(eval_all):\n",
    "\n",
    "    data_eval = [[\"WordSim_sim\", eval_all[0][0], eval_all[1][0], eval_all[2][0]\n",
    "    ],\n",
    "    [\"WordSim_rel\", eval_all[0][1], eval_all[1][1], eval_all[2][1]\n",
    "    ],\n",
    "    [\"WordSim_353\", eval_all[0][2], eval_all[1][2], eval_all[2][2]\n",
    "    ],\n",
    "    [\"MEN\", eval_all[0][3], eval_all[1][3], eval_all[2][3]\n",
    "    ],\n",
    "    [\"M.Turk\", eval_all[0][4], eval_all[1][4], eval_all[2][4]\n",
    "    ],\n",
    "    [\"SimLex\", eval_all[0][5], eval_all[1][5], eval_all[2][5]\n",
    "    ],\n",
    "    [\"Analogy_sem\", eval_all[0][6], eval_all[1][6], eval_all[2][6]\n",
    "    ],\n",
    "    [\"Analogy_syn\", eval_all[0][7], eval_all[1][7], eval_all[2][7]\n",
    "    ]\n",
    "    ]\n",
    "\n",
    "    return data_eval\n",
    "\n",
    "colnames = [\"evals\", \"wv_50\",\"ft_50\",\"gv_50\"]\n",
    "# cp_name = 'brown'\n",
    "cp_name = 'text8'\n",
    "res_eval = eval_table_oneDim(eval_all[cp_name])\n",
    "df = pd.DataFrame(res_eval, columns = colnames)\n",
    "\n",
    "(df.style\n",
    "    .applymap(color_negative_red, subset = pd.IndexSlice[:, colnames[1:len(colnames)]])\n",
    "    .apply(highlight_max, axis= 1, subset = pd.IndexSlice[0:6, colnames[1:len(colnames)]]\n",
    "    .format({\n",
    "        'wv_50': '{:,.3f}'.format,\n",
    "        'ft_50': '{:,.3f}'.format,\n",
    "        'gv_50': '{:,.3f}'.format,\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_all['text8'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table(eval_all):\n",
    "    \n",
    "    data_eval = [[\"WordSim_sim\",\n",
    "    eval_all[0][0], eval_all[1][0], eval_all[2][0], eval_all[3][0], eval_all[4][0],\n",
    "    eval_all[5][0], eval_all[6][0], eval_all[7][0], eval_all[8][0], eval_all[9][0],\n",
    "    eval_all[10][0], eval_all[11][0], eval_all[12][0], eval_all[13][0], eval_all[14][0]\n",
    "    ],\n",
    "    [\"WordSim_rel\",\n",
    "    eval_all[0][1], eval_all[1][1], eval_all[2][1], eval_all[3][1], eval_all[4][1],\n",
    "    eval_all[5][1], eval_all[6][1], eval_all[7][1], eval_all[8][1], eval_all[9][1],\n",
    "    eval_all[10][1], eval_all[11][1], eval_all[12][1], eval_all[13][1], eval_all[14][1]\n",
    "    ],\n",
    "    [\"WordSim_353\",\n",
    "    eval_all[0][2], eval_all[1][2], eval_all[2][2], eval_all[3][2], eval_all[4][2],\n",
    "    eval_all[5][2], eval_all[6][2], eval_all[7][2], eval_all[8][2], eval_all[9][2],\n",
    "    eval_all[10][2], eval_all[11][2], eval_all[12][2], eval_all[13][2], eval_all[14][2]\n",
    "    ],\n",
    "    [\"MEN\",\n",
    "    eval_all[0][3], eval_all[1][3], eval_all[2][3], eval_all[3][3], eval_all[4][3],\n",
    "    eval_all[5][3], eval_all[6][3], eval_all[7][3], eval_all[8][3], eval_all[9][3],\n",
    "    eval_all[10][3], eval_all[11][3], eval_all[12][3], eval_all[13][3], eval_all[14][3]\n",
    "    ],\n",
    "    [\"M.Turk\",\n",
    "    eval_all[0][4], eval_all[1][4], eval_all[2][4], eval_all[3][4], eval_all[4][4],\n",
    "    eval_all[5][4], eval_all[6][4], eval_all[7][4], eval_all[8][4], eval_all[9][4],\n",
    "    eval_all[10][4], eval_all[11][4], eval_all[12][4], eval_all[13][4], eval_all[14][4]\n",
    "    ],\n",
    "    [\"SimLex\",\n",
    "    eval_all[0][5], eval_all[1][5], eval_all[2][5], eval_all[3][5], eval_all[4][5],\n",
    "    eval_all[5][5], eval_all[6][5], eval_all[7][5], eval_all[8][5], eval_all[9][5],\n",
    "    eval_all[10][5], eval_all[11][5], eval_all[12][5], eval_all[13][5], eval_all[14][5]\n",
    "    ],\n",
    "    [\"Google_sem\",\n",
    "    eval_all[0][6], eval_all[1][6], eval_all[2][6], eval_all[3][6], eval_all[4][6],\n",
    "    eval_all[5][6], eval_all[6][6], eval_all[7][6], eval_all[8][6], eval_all[9][6],\n",
    "    eval_all[10][6], eval_all[11][6], eval_all[12][6], eval_all[13][6], eval_all[14][6]\n",
    "    ],\n",
    "    [\"Google_syn\",\n",
    "    eval_all[0][7], eval_all[1][7], eval_all[2][7], eval_all[3][7], eval_all[4][7],\n",
    "    eval_all[5][7], eval_all[6][7], eval_all[7][7], eval_all[8][7], eval_all[9][7],\n",
    "    eval_all[10][7], eval_all[11][7], eval_all[12][7], eval_all[13][7], eval_all[14][7]\n",
    "    ]\n",
    "    ]\n",
    "\n",
    "    return data_eval\n",
    "colnames = [\"evals\", \"wv_50\", \"wv_100\", \"wv_150\", \"wv_200\", \"wv_250\",\n",
    "    \"ft_50\", \"ft_100\", \"ft_150\", \"ft_200\", \"ft_250\",\n",
    "    \"gv_50\", \"gv_100\", \"gv_150\", \"gv_200\", \"gv_250\"\n",
    "]\n",
    "# cp_name = 'brown'\n",
    "cp_name = 'text8'\n",
    "res_eval = eval_table(eval_all[cp_name])\n",
    "df = pd.DataFrame(res_eval, columns = colnames)\n",
    "(df.style\n",
    "    .applymap(color_negative_red, subset = pd.IndexSlice[:, colnames[1:len(colnames)]])\n",
    "    .apply(highlight_max, axis= 1, subset = pd.IndexSlice[0:6, colnames[1:len(colnames)]])\n",
    "    .format({\n",
    "        'wv_50': '{:,.3f}'.format,\n",
    "        'wv_100': '{:,.3f}'.format,\n",
    "        'wv_150': '{:,.3f}'.format,\n",
    "        'wv_200': '{:,.3f}'.format,\n",
    "        'wv_250': '{:,.3f}'.format,\n",
    "        'ft_50': '{:,.3f}'.format,\n",
    "        'ft_100': '{:,.3f}'.format,\n",
    "        'ft_150': '{:,.3f}'.format,\n",
    "        'ft_200': '{:,.3f}'.format,\n",
    "        'ft_250': '{:,.3f}'.format,\n",
    "        'gv_50': '{:,.3f}'.format,\n",
    "        'gv_100': '{:,.3f}'.format,\n",
    "        'gv_150': '{:,.3f}'.format,\n",
    "        'gv_200': '{:,.3f}'.format,\n",
    "        'gv_250': '{:,.3f}'.format,\n",
    "        })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the WE from Word2Vec (on Text8)\n",
    "\n",
    "# and evaluate\n",
    "# ----------------\n",
    "MODELS_DIR = '../data/models/'\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import os\n",
    "all_models = dict()\n",
    "# Load the WE\n",
    "for dim in [100, 150, 200, 250]:\n",
    "    tmp_path = os.path.join(MODELS_DIR, 'text8_wv{:d}dim.vec'.format(dim))\n",
    "    if os.path.exists(tmp_path):\n",
    "        print(\"Load embeddings from\", 'text8_wv{:d}dim.vec'.format(dim))\n",
    "        #gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "        all_models[str(dim)] = KeyedVectors.load_word2vec_format(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference vocabulary for the concatenation\n",
    "ref_vocab_model = all_models[\"100\"]\n",
    "# Get the ref_vocab with word in decreasing order of frequency\n",
    "ref_vocab_dict = dict()\n",
    "for item in ref_vocab_model.vocab:\n",
    "    ref_vocab_dict[item] = ref_vocab_model.vocab[item].count\n",
    "ref_vocab_dictSorted=dict(sorted(ref_vocab_dict.items(), key=lambda x: x[1],reverse=True))\n",
    "ref_vocab_size = len(ref_vocab_dictSorted)\n",
    "print(ref_vocab_size)\n",
    "#print(ref_vocab_dictSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the word vectors into a numpy matrix\n",
    "# follow the reference vocabulary to build the matrix\n",
    "all_embedding_matrix = dict()\n",
    "print(\"\\n Converting...\\n\")\n",
    "for dim in [100, 150, 200, 250]:\n",
    "    print(\"{:d}\".format(dim))\n",
    "    tmp_vocab_size = len(all_models[\"{:d}\".format(dim)].vocab)\n",
    "    print(\"size vocab -->\", tmp_vocab_size)\n",
    "    if not tmp_vocab_size == len(ref_vocab_dict):\n",
    "        print(\"--!!--> Difference voc. sizes ({:d} while ref. {:d})\".format(tmp_vocab_size, ref_vocab_size))\n",
    "   \n",
    "    all_embedding_matrix[\"{:d}\".format(dim)] = np.zeros((ref_vocab_size, dim))\n",
    "    for str_w in ref_vocab_dictSorted:\n",
    "        tmp_model = all_models[\"{:d}\".format(dim)]\n",
    "        tmp_embedding_vector = tmp_model[str_w]\n",
    "        if tmp_embedding_vector is not None:\n",
    "            all_embedding_matrix[\"{:d}\".format(dim)][ref_vocab_dictSorted[str_w]-1] = tmp_embedding_vector\n",
    "        else:\n",
    "            print(\"{:s} embedding is None\".format(str_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the word vectors into a numpy matrix that is\n",
    "## suitable for insertion into our TensorFlow and Keras models\n",
    "#all_embedding_matrix = dict()\n",
    "#all_vocab = dict()\n",
    "#\n",
    "#for dim in [100, 150, 200, 250]:\n",
    "# print(dim)\n",
    "# tmp_vocab_size = len(all_models[str(dim)].vocab)\n",
    "# print(\"size vocab -->\", tmp_vocab_size)\n",
    "#\n",
    "# all_embedding_matrix[str(dim)] = np.zeros((tmp_vocab_size, dim))\n",
    "# all_vocab[str(dim)] = ['']*tmp_vocab_size\n",
    "#\n",
    "# for i in range(tmp_vocab_size):\n",
    "# tmp_model = all_models[str(dim)]\n",
    "# tmp_embedding_vector = tmp_model[tmp_model.index2word[i]]\n",
    "#\n",
    "# if tmp_embedding_vector is not None:\n",
    "# all_embedding_matrix[str(dim)][i] = tmp_embedding_vector\n",
    "# all_vocab[str(dim)][i] = tmp_model.index2word[i]\n",
    "# else:\n",
    "# print(tmp_model.index2word[i], \"None\")\n",
    "#\n",
    "#all_vocab_ok = True\n",
    "#dim = 100\n",
    "#ref_vocab = all_vocab[str(dim)]\n",
    "#print(\"\\nLength ref_vocab\", len(ref_vocab))\n",
    "#print(\"Dim\", dim, ref_vocab[0:10], \"\\n\")\n",
    "#\n",
    "#for dim in [150, 200, 250]:\n",
    "# print(\"Dim\", dim, all_vocab[str(dim)][0:10])\n",
    "#\n",
    "# if not all_vocab[str(dim)] == ref_vocab:\n",
    "# print(\"Not all vocabulary identical\")\n",
    "# all_vocab_ok = False\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the embeddings of all_models\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "all_embedding_concat_matrix = all_embedding_matrix[str(100)]\n",
    "\n",
    "for dim in [150, 200, 250]:\n",
    "    print(dim)\n",
    "    all_embedding_concat_matrix = np.concatenate((all_embedding_concat_matrix, all_embedding_matrix[str(dim)]), axis = 1)\n",
    "    \n",
    "print(all_embedding_concat_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write the concatenate the embeddings of all_models\n",
    "concat_file = os.path.join(MODELS_DIR, 'concat_wv_models.vec')\n",
    "\n",
    "if not os.path.isfile(concat_file):\n",
    "    \n",
    "    # Save vectors to output file\n",
    "    with open(concat_file, 'w+') as f:\n",
    "        f.write(str(all_embedding_concat_matrix.shape[0]) + \" \" + str(all_embedding_concat_matrix.shape[1]) + \"\\n\")\n",
    "\n",
    "        for word in ref_vocab_dictSorted:\n",
    "            word_id = (ref_vocab_dictSorted[word]-1)\n",
    "            f.write(word + \" \" + \" \".join([str(value) for value in all_embedding_concat_matrix[word_id]]) + \"\\n\")\n",
    "        \n",
    "    \n",
    "    print('\\nSaved concatenated embeddings model as concat_wv_models.vec')\n",
    "else:\n",
    "    print('\\nUsing existing model file concat_wv_models.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLoading embeddings of', concat_file)\n",
    "concat_model = KeyedVectors.load_word2vec_format(concat_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_analogy_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ea697b8aeb6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconcat_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mret_sem_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret_syn_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint_analogy_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_analogies_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mconcat_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret_sem_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mconcat_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret_syn_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SimLex-999 similarity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_analogy_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "concat_eval = list()\n",
    "print('Accuracy:')\n",
    "ret_sem_acc, ret_syn_acc = print_analogy_accuracy(concat_model, word_analogies_file)\n",
    "concat_eval.append(ret_sem_acc); concat_eval.append(ret_syn_acc)\n",
    "print('SimLex-999 similarity')\n",
    "ret_corSpearman_simlex = print_similarity_accuracy(concat_model, simlex_file)\n",
    "concat_eval.append(ret_corSpearman_simlex)\n",
    "print(\"\\nWordSim-353\")\n",
    "ret_corSpearman_word = print_similarity_accuracy(concat_model, wordsim_file)\n",
    "concat_eval.append(ret_corSpearman_word)\n",
    "print(\"\\nWordSim-353 relatedness\")\n",
    "ret_corSpearman_word_rel = print_similarity_accuracy(concat_model, wordsim_rel_file)\n",
    "concat_eval.append(ret_corSpearman_word_rel)\n",
    "print(\"\\nWordSim-353 similarity\")\n",
    "ret_corSpearman_word_sim = print_similarity_accuracy(concat_model, wordsim_sim_file)\n",
    "concat_eval.append(ret_corSpearman_word_sim)\n",
    "print(\"\\nMEN similarity\")\n",
    "ret_corSpearman_men = print_similarity_accuracy(concat_model, men_file)\n",
    "concat_eval.append(ret_corSpearman_men)\n",
    "print(\"\\nMturk similarity\")\n",
    "ret_corSpearman_men = print_similarity_accuracy(concat_model, mturk_file)\n",
    "concat_eval.append(ret_corSpearman_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table_singleMethod_concat(eval_all):\n",
    "    data_eval = [[\"QWsem\", eval_all[0], \n",
    "                 ],\n",
    "\n",
    "                 [\"QWsyn\", eval_all[1], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_simlex\", eval_all[2], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word\", eval_all[3], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word_rel\", eval_all[4], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word_sim\", eval_all[5], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_men\", eval_all[6], \n",
    "                 ]\n",
    "                ]\n",
    "\n",
    "\n",
    "    return data_eval\n",
    "\n",
    "colnames = [\"evals\", \"wv_dim_concat\", \n",
    "               ]\n",
    "\n",
    "# cp_name = 'brown'\n",
    "cp_name = 'text8'\n",
    "res_eval = eval_table_singleMethod_concat(concat_eval)\n",
    "df = pd.DataFrame(res_eval, columns = colnames)\n",
    "\n",
    "(df.style\n",
    "   .applymap(color_negative_red, subset = pd.IndexSlice[:, colnames[1:len(colnames)]])\n",
    "   .apply(highlight_max, axis= 1, subset = pd.IndexSlice[0:6, colnames[1:len(colnames)]])\n",
    "   .format({\n",
    "        'wv_dim_concat': '{:,.3f}'.format,\n",
    "})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the WE from fastText (on Text8)\n",
    "# and evaluate\n",
    "# ----------------\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "all_models = dict()\n",
    "\n",
    "# Load the WE\n",
    "for dim in [100, 150, 200, 250]:\n",
    "    tmp_path = os.path.join(MODELS_DIR, 'text8_ft{:d}dim.vec'.format(dim))\n",
    "    \n",
    "    if os.path.exists(tmp_path):\n",
    "        print(\"Load embeddings from\", 'text8_ft{:d}dim.vec'.format(dim))\n",
    "        #gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "        all_models[str(dim)] = KeyedVectors.load_word2vec_format(tmp_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference vocabulary for the concatenation\n",
    "ref_vocab_model = all_models[\"100\"]\n",
    "\n",
    "# Get the ref_vocab with word in decreasing order of frequency\n",
    "ref_vocab_dict = dict()\n",
    "for item in ref_vocab_model.vocab:\n",
    "    ref_vocab_dict[item] = ref_vocab_model.vocab[item].count\n",
    "    \n",
    "ref_vocab_dictSorted=dict(sorted(ref_vocab_dict.items(), key=lambda x: x[1],reverse=True))\n",
    "ref_vocab_size = len(ref_vocab_dictSorted)\n",
    "print(ref_vocab_size)\n",
    "#print(ref_vocab_dictSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the word vectors into a numpy matrix\n",
    "# follow the reference vocabulary to build the matrix\n",
    "all_embedding_matrix = dict()\n",
    "\n",
    "print(\"\\n Converting...\\n\")\n",
    "for dim in [50, 100, 150, 200, 250]:\n",
    "        \n",
    "    print(\"{:d}\".format(dim))\n",
    "    tmp_vocab_size = len(all_models[\"{:d}\".format(dim)].vocab)\n",
    "    print(\"size vocab -->\", tmp_vocab_size)\n",
    "    \n",
    "    if not tmp_vocab_size == len(ref_vocab_dict):\n",
    "        print(\"--!!--> Difference voc. sizes ({:d} while ref. {:d})\".format(tmp_vocab_size, ref_vocab_size))\n",
    "\n",
    "    all_embedding_matrix[\"{:d}\".format(dim)] = np.zeros((ref_vocab_size, dim))\n",
    "\n",
    "    for str_w in ref_vocab_dictSorted:\n",
    "        tmp_model = all_models[\"{:d}\".format(dim)]\n",
    "        tmp_embedding_vector = tmp_model[str_w]\n",
    "\n",
    "        if tmp_embedding_vector is not None:\n",
    "            all_embedding_matrix[\"{:d}\".format(dim)][ref_vocab_dictSorted[str_w]-1] = tmp_embedding_vector\n",
    "        else:\n",
    "            print(\"{:s} embedding is None\".format(str_w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the word vectors into a numpy matrix that is \n",
    "## suitable for insertion into our TensorFlow and Keras models\n",
    "#all_embedding_matrix = dict()\n",
    "#all_vocab = dict()\n",
    "#\n",
    "#for dim in [100, 150, 200, 250]:\n",
    "#    print(dim)\n",
    "#    tmp_vocab_size = len(all_models[str(dim)].vocab)\n",
    "#    print(\"size vocab -->\", tmp_vocab_size)\n",
    "#\n",
    "#    all_embedding_matrix[str(dim)] = np.zeros((tmp_vocab_size, dim))\n",
    "#    all_vocab[str(dim)] = ['']*tmp_vocab_size\n",
    "#    \n",
    "#    for i in range(tmp_vocab_size):\n",
    "#        tmp_model = all_models[str(dim)]\n",
    "#        tmp_embedding_vector = tmp_model[tmp_model.index2word[i]]\n",
    "#\n",
    "#        if tmp_embedding_vector is not None:\n",
    "#            all_embedding_matrix[str(dim)][i] = tmp_embedding_vector\n",
    "#            all_vocab[str(dim)][i] = tmp_model.index2word[i]\n",
    "#        else:\n",
    "#            print(tmp_model.index2word[i], \"None\")\n",
    "#\n",
    "#all_vocab_ok = True\n",
    "#dim = 100\n",
    "#ref_vocab = all_vocab[str(dim)]\n",
    "#print(\"\\nLength ref_vocab\", len(ref_vocab))\n",
    "#print(\"Dim\", dim, ref_vocab[0:10], \"\\n\")\n",
    "#\n",
    "#for dim in [150, 200, 250]:\n",
    "#    print(\"Dim\", dim, all_vocab[str(dim)][0:10])\n",
    "#    \n",
    "#    if not all_vocab[str(dim)] == ref_vocab:\n",
    "#        print(\"Not all vocabulary identical\")\n",
    "#        all_vocab_ok = False\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the embeddings of all_models\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "all_embedding_concat_matrix = all_embedding_matrix[str(100)]\n",
    "\n",
    "for dim in [150, 200, 250]:\n",
    "    print(dim)\n",
    "    all_embedding_concat_matrix = np.concatenate((all_embedding_concat_matrix, all_embedding_matrix[str(dim)]), axis = 1)\n",
    "    \n",
    "print(all_embedding_concat_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the concatenate the embeddings of all_models\n",
    "concat_file = os.path.join(MODELS_DIR, 'concat_ft_models.vec')\n",
    "\n",
    "if not os.path.isfile(concat_file):\n",
    "    \n",
    "    # Save vectors to output file\n",
    "    with open(concat_file, 'w+') as f:\n",
    "        f.write(str(all_embedding_concat_matrix.shape[0]) + \" \" + str(all_embedding_concat_matrix.shape[1]) + \"\\n\")\n",
    "\n",
    "        for word in ref_vocab_dictSorted:\n",
    "            word_id = (ref_vocab_dictSorted[word]-1)\n",
    "            f.write(word + \" \" + \" \".join([str(value) for value in all_embedding_concat_matrix[word_id]]) + \"\\n\")\n",
    "        \n",
    "    \n",
    "    print('\\nSaved concatenated embeddings model as concat_ft_models.vec')\n",
    "else:\n",
    "    print('\\nUsing existing model file concat_ft_models.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLoading embeddings of', concat_file)\n",
    "concat_model = KeyedVectors.load_word2vec_format(concat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_eval = list()\n",
    "print('Accuracy:')\n",
    "ret_sem_acc, ret_syn_acc = print_analogy_accuracy(concat_model, word_analogies_file)\n",
    "concat_eval.append(ret_sem_acc); concat_eval.append(ret_syn_acc)\n",
    "print('SimLex-999 similarity')\n",
    "ret_corSpearman_simlex = print_similarity_accuracy(concat_model, simlex_file)\n",
    "concat_eval.append(ret_corSpearman_simlex)\n",
    "print(\"\\nWordSim-353\")\n",
    "ret_corSpearman_word = print_similarity_accuracy(concat_model, wordsim_file)\n",
    "concat_eval.append(ret_corSpearman_word)\n",
    "print(\"\\nWordSim-353 relatedness\")\n",
    "ret_corSpearman_word_rel = print_similarity_accuracy(concat_model, wordsim_rel_file)\n",
    "concat_eval.append(ret_corSpearman_word_rel)\n",
    "print(\"\\nWordSim-353 similarity\")\n",
    "ret_corSpearman_word_sim = print_similarity_accuracy(concat_model, wordsim_sim_file)\n",
    "concat_eval.append(ret_corSpearman_word_sim)\n",
    "print(\"\\nMEN similarity\")\n",
    "ret_corSpearman_men = print_similarity_accuracy(concat_model, men_file)\n",
    "concat_eval.append(ret_corSpearman_men)\n",
    "print(\"\\nMturk similarity\")\n",
    "ret_corSpearman_men = print_similarity_accuracy(concat_model, mturk_file)\n",
    "concat_eval.append(ret_corSpearman_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table_singleMethod_concat(eval_all):\n",
    "    data_eval = [[\"QWsem\", eval_all[0], \n",
    "                 ],\n",
    "\n",
    "                 [\"QWsyn\", eval_all[1], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_simlex\", eval_all[2], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word\", eval_all[3], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word_rel\", eval_all[4], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word_sim\", eval_all[5], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_men\", eval_all[6], \n",
    "                 ]\n",
    "                ]\n",
    "\n",
    "\n",
    "    return data_eval\n",
    "\n",
    "colnames = [\"evals\", \"ft_dim_concat\", \n",
    "               ]\n",
    "\n",
    "# cp_name = 'brown'\n",
    "cp_name = 'text8'\n",
    "res_eval = eval_table_singleMethod_concat(concat_eval)\n",
    "df = pd.DataFrame(res_eval, columns = colnames)\n",
    "\n",
    "(df.style\n",
    "   .applymap(color_negative_red, subset = pd.IndexSlice[:, colnames[1:len(colnames)]])\n",
    "   .apply(highlight_max, axis= 1, subset = pd.IndexSlice[0:6, colnames[1:len(colnames)]])\n",
    "   .format({\n",
    "        'ft_dim_concat': '{:,.3f}'.format,\n",
    "})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the WE from word2vec AND fastText (on Text8)\n",
    "# and evaluate\n",
    "# ----------------\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "all_models = dict()\n",
    "all_dim = [100, 150, 200, 250]\n",
    "all_meth_dim = list(zip(['wv']*len(all_dim)+['ft']*len(all_dim), all_dim*2))\n",
    "\n",
    "# Load the WE\n",
    "for meth, dim in all_meth_dim:\n",
    "    \n",
    "    tmp_path = os.path.join(MODELS_DIR, 'text8_{:s}{:d}dim.vec'.format(meth, dim))\n",
    "    \n",
    "    if os.path.exists(tmp_path):\n",
    "        print(\"Load embeddings from\", 'text8_{:s}{:d}dim.vec'.format(meth, dim))\n",
    "        #gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "        all_models[\"{:s}_{:d}\".format(meth, dim)] = KeyedVectors.load_word2vec_format(tmp_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_vocab_model = all_models[\"wv_100\"]\n",
    "\n",
    "# Get the ref_vocab with word in decreasing order of frequency\n",
    "ref_vocab_dict = dict()\n",
    "for item in ref_vocab_model.vocab:\n",
    "    ref_vocab_dict[item] = ref_vocab_model.vocab[item].count\n",
    "    \n",
    "ref_vocab_dictSorted=dict(sorted(ref_vocab_dict.items(), key=lambda x: x[1],reverse=True))\n",
    "ref_vocab_size = len(ref_vocab_dictSorted)\n",
    "\n",
    "print(len(ref_vocab_dictSorted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the word vectors into a numpy matrix\n",
    "# follow the reference vocabulary to build the matrix\n",
    "all_embedding_matrix = dict()\n",
    "\n",
    "print(\"\\n Converting...\\n\")\n",
    "for meth, dim in all_meth_dim:\n",
    "        \n",
    "    print(\"{:s}_{:d}\".format(meth, dim))\n",
    "    tmp_vocab_size = len(all_models[\"{:s}_{:d}\".format(meth, dim)].vocab)\n",
    "    print(\"size vocab -->\", tmp_vocab_size)\n",
    "    \n",
    "    if not tmp_vocab_size == len(ref_vocab_dict):\n",
    "        print(\"--!!--> Difference voc. sizes ({:d} while ref. {:d})\".format(tmp_vocab_size, ref_vocab_size))\n",
    "\n",
    "    all_embedding_matrix[\"{:s}_{:d}\".format(meth, dim)] = np.zeros((ref_vocab_size, dim))\n",
    "\n",
    "    for str_w in ref_vocab_dictSorted:\n",
    "        tmp_model = all_models[\"{:s}_{:d}\".format(meth, dim)]\n",
    "        #tmp_embedding_vector = tmp_model[tmp_model.index2word[i]]\n",
    "        tmp_embedding_vector = tmp_model[str_w]\n",
    "\n",
    "        if tmp_embedding_vector is not None:\n",
    "            all_embedding_matrix[\"{:s}_{:d}\".format(meth, dim)][ref_vocab_dictSorted[str_w]-1] = tmp_embedding_vector\n",
    "        else:\n",
    "            print(\"{:s} embedding is None\".foramt(str_w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the embeddings of all_models\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "all_embedding_concat_matrix = all_embedding_matrix[\"wv_100\"]\n",
    "\n",
    "for meth, dim in all_meth_dim:\n",
    "    print(\"{:s}_{:d}\".format(meth, dim))\n",
    "    all_embedding_concat_matrix = np.concatenate((all_embedding_concat_matrix, all_embedding_matrix[\"{:s}_{:d}\".format(meth, dim)]), axis = 1)\n",
    "    \n",
    "print(all_embedding_concat_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the concatenate the embeddings of all_models\n",
    "concat_file = os.path.join(MODELS_DIR, 'concat_wv_ft_models.vec')\n",
    "\n",
    "if not os.path.isfile(concat_file):\n",
    "    \n",
    "    # Save vectors to output file\n",
    "    with open(concat_file, 'w+') as f:\n",
    "        f.write(str(all_embedding_concat_matrix.shape[0]) + \" \" + str(all_embedding_concat_matrix.shape[1]) + \"\\n\")\n",
    "\n",
    "        for word in ref_vocab_dictSorted:\n",
    "            word_id = (ref_vocab_dictSorted[word]-1)\n",
    "            f.write(word + \" \" + \" \".join([str(value) for value in all_embedding_concat_matrix[word_id]]) + \"\\n\")\n",
    "        \n",
    "    \n",
    "    print('\\nSaved concatenated embeddings model as concat_wv_ft_models.vec')\n",
    "else:\n",
    "    print('\\nUsing existing model file concat_wv_ft_models.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLoading embeddings of', concat_file)\n",
    "concat_model = KeyedVectors.load_word2vec_format(concat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_eval = list()\n",
    "print('Accuracy:')\n",
    "ret_sem_acc, ret_syn_acc = print_analogy_accuracy(concat_model, word_analogies_file)\n",
    "concat_eval.append(ret_sem_acc); concat_eval.append(ret_syn_acc)\n",
    "print('SimLex-999 similarity')\n",
    "ret_corSpearman_simlex = print_similarity_accuracy(concat_model, simlex_file)\n",
    "concat_eval.append(ret_corSpearman_simlex)\n",
    "print(\"\\nWordSim-353\")\n",
    "ret_corSpearman_word = print_similarity_accuracy(concat_model, wordsim_file)\n",
    "concat_eval.append(ret_corSpearman_word)\n",
    "print(\"\\nWordSim-353 relatedness\")\n",
    "ret_corSpearman_word_rel = print_similarity_accuracy(concat_model, wordsim_rel_file)\n",
    "concat_eval.append(ret_corSpearman_word_rel)\n",
    "print(\"\\nWordSim-353 similarity\")\n",
    "ret_corSpearman_word_sim = print_similarity_accuracy(concat_model, wordsim_sim_file)\n",
    "concat_eval.append(ret_corSpearman_word_sim)\n",
    "print(\"\\nMEN similarity\")\n",
    "ret_corSpearman_men = print_similarity_accuracy(concat_model, men_file)\n",
    "concat_eval.append(ret_corSpearman_men)\n",
    "print(\"\\nMturk similarity\")\n",
    "ret_corSpearman_men = print_similarity_accuracy(concat_model, mturk_file)\n",
    "concat_eval.append(ret_corSpearman_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table_singleMethod_concat(eval_all):\n",
    "    data_eval = [[\"QWsem\", eval_all[0], \n",
    "                 ],\n",
    "\n",
    "                 [\"QWsyn\", eval_all[1], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_simlex\", eval_all[2], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word\", eval_all[3], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word_rel\", eval_all[4], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_word_sim\", eval_all[5], \n",
    "                 ],\n",
    "\n",
    "                 [\"CorSPear_men\", eval_all[6], \n",
    "                 ]\n",
    "                ]\n",
    "\n",
    "\n",
    "    return data_eval\n",
    "\n",
    "colnames = [\"evals\", \"wv_ft_dim_concat\", \n",
    "               ]\n",
    "\n",
    "# cp_name = 'brown'\n",
    "cp_name = 'text8'\n",
    "res_eval = eval_table_singleMethod_concat(concat_eval)\n",
    "df = pd.DataFrame(res_eval, columns = colnames)\n",
    "\n",
    "(df.style\n",
    "   .applymap(color_negative_red, subset = pd.IndexSlice[:, colnames[1:len(colnames)]])\n",
    "   .apply(highlight_max, axis= 1, subset = pd.IndexSlice[0:6, colnames[1:len(colnames)]])\n",
    "   .format({\n",
    "        'wv_ft_dim_concat': '{:,.3f}'.format,\n",
    "})\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
